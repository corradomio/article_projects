

-----------------------------------------------------------------------------
- Whatsapp
-----------------------------------------------------------------------------

[Gabriele]
ciao, abbiamo due aspetti da sviluppare Bayesian Optimization e Data Distillation. Dell'aspetto Ottimizzazione
Bayesiana abbiamo gi√† detto (ho condiviso anche a Gianni la cartella /Gabriele_corrado); riguardo alla Distillation,
io intendevo focalizzarmi sul sotto-problema della "coreset selection" (mentre la distillation comprende anche la
creazione di esempi rappresentativi artefatti, la coreset selection si limita, nella sua accezione restrittiva a
selezionare solo esempi gi√† esistenti nel dataset originario). Ci sono un sacco di articoli in giro con vari obiettivi
e differenze nelle definizioni (pi√π o meno ampie), ma credo di avere trovato il nostro bandolo della matassa: si tratta
dell'articolo "DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning" di Guo et al., che hanno
raccolto e implementato anche con PyTorch tutti i metodi principali e forniscono anche un sacco di benchmark, cos√¨ da
consentirci di avere gi√† un ampio spettro di termini di confronto. Altro articolo che fornisce dei benchmark dataset
(ma non codice) √® "DC-BENCH: Dataset Condensation Benchmark" di Cui ed al. Li salvo entrambi nella cartella condivisa.


iniziando con deep core si ha una visione del problema della coreset selection, in parallelo o separatamente uno pu√≤
guardare per la bayesian optimization la molto figurativa introduzione fornita dal libro di Nguyen e l'a chiarissima
e formale presentazioni di Candelieri (entrambe queste ultime salvate adesso nella cartella di corrado)


ho aggiunto anche il libro del mio senior collega Archetti, il cui capitolo 6 presenta un bel po' di strumenti software
per la Bayesian optimization

[Gabriele]
Ho salvato nella cartella un altro papiro (che avevo gi√† mostrato luned√¨): "Efficient Black-Box Combinatorial
Optimization" del 2020. Due cose vanno dette: 1) mi ha fregato l'idea di usare lo shapley/banzaf value per selezionare
il subset (come avevamo fatto nella tesi di Corrado), infatti il papiro usa la base di Fourier invece della base di
Moebius, quindi semplicemente codifica presenza assenza con +1 e -1 anzich√© con 1 e 0, ma strutturalmente √® la stessa
idea 2) la buona notizia √® che non l'ha notato quasi nessuno, cio√® il papiro non √® citato da altri che ci costruiscano
sopra, ma solo nei related work di lavori sulla black-box optimization. Quindi c'√® spazio per costruirci sopra,
riversando anche la nostra expertise in "Shapleyologia comparata" üòâ


L'idea di fondo √® questa: noi vogliamo ottimizzare una set function (che √® black box). Ad ogni valutazione (dispendiosa)
aggiorniamo il modello probabilistico (il surrogate model che adesso descrivo) e poi scegliamo il prossimo insieme
usando un'acquisition function opportuna (ad esempio la Expected Improvement). Il modello probabilistico in Bayesian
Optimization √® una distribuzione di probabilit√† sulle possibili funzioni pseudo-booleane (con n punti candidati a far
parte dell'insieme ottimale abbiamo funzioni 2^n-->R). Questo modello a rigore abiterebbe in uno spazio 2^n
dimensionale, ma noi lo approssimiamo con lo Shapley value dei singoli punti, quindi lavoriamo in uno spazio n
dimensionale, e se vogliamo anche usare l'interaction value aggiungiamo n(n-1)/2 dimensioni. Il resto √® procedura
standard (che estrinseco quando ci vediamo).


Va detto che noi non effettuiamo una ricerca sull'intero reticolo booleano, ma abbiamo come obiettivo quello di trovare
un insieme di cardinalit√† k<<n, appunto il coreset; quindi possiamo usare il banzaf ridotto, come nella teso di Corrado
(ecco un'altra novit√† rispetto al papiro salvato), ma con un numero di candidati n grande il calcolo √® comunque
proibitivo).


Adesso penso a come sfruttare un fatto nuovo rispetto alla tesi di Corrado, una "mossa magica": qui possiamo fare il
training sull'intero campione di un modello di Machine Learning trasparente, come un Decision Tree, cos√¨ da ottenere
informazioni sulle frontiere tra le regioni e poi cercare i punti candidati proprio l√¨ vicino, perch√© √® l√¨ che ci
saranno i punti pi√π informativi. Questa √® la mi idea ad oggi, e si differenzia dal papiro salvato perch√© quello parla
di ottimizzazione compibatoria generica, mentre qui noi abbiamo a che fare con un caso molto particolare, grazie alla
"mossa magica".


La mossa magica pu√≤ sembrare una mossa truffaldina, ma non lo √®. Infatti l'obiettivo dell'ottimizzazione √® ottenere un
insieme (di k punti estratto degli n, ad esempio con con n=10k) sfruttando un certo budget di calcolo. Ora √® noto che
un round di training implica un effort direttamente proporzionale al numero di esempi utilizzato: indico il budget con
b x n, dove b √® il budget consumato per singolo esempio. Se mi danno un budget b x (2 n), io posso usare il budget b x n
per la singola valutazione dell'intero training dataset, e 10 volte b k per valutare dieci insiemi candidati, ciascuno
di k punti: quindi dopo la mossa magica ho 10 shot grazie ai quali valutare la funzione obiettivo (10 iterazioni dellla
Bayesian Optimization). Questi numeri sono solo esemplificativi. Il concetto √® che io posso spezzare il budget in modo
ottimale tra mossa magica e numero di iterazioni consentite.


Questo da luogo ad un problema interessante, e cio√® come si spezza in modo ottimale il budget.


Sembrerebbe un problema di ottimizzazione ad un terzo livello (forse semplice se la funzione associata al budget √®
convessa): il livello pi√π esterno √® ottimizzare il budget utilizzando campioni di dimensione ad esempio decrescente,
poi usare l'ottimizzazione bayesiana per scegliere il campione di un certo livello k, poi all'interno di ogni singola
iterazione dell'ottimizzazione bayesiana ottimizzare la funzione d'acquisizione (anche questo sembra un problema
semplice).


Tutto ci√≤ senza assumere alcuna struttura nello spazio dei dati di input. Ma se ciascun punto x = (x_1,...,x_D)
appartiene ad uno spazio D-dimensionale (ad esempio Euclideo) si pu√≤ cercare un embedding in uno spazio d-dimensionale
con d<D (il che aiuta sicuramente la ricerca) anche senza ricorrere minimamente alla dispendiosa valutazione della
funzione f(S).


Siccome il dataset da condensare contiene le coppie (x,y) (cio√® le coppie (input, label)), l'embedding √® particolarmente
informativo e si sovrappone alla "mossa magica", cio√® al primo round di classificazione che utilizza tutto il campione.
Come sfruttare questo fatto, ad esempio con delle SVM, √® da chiarire.


Ho inoltre un'osservazione riguardo a quanto avevo scritto sopra: mi √® stata chiara fin da domenica, mentre viaggiavo
sul flixbus per Lione, senza connessione internet, quindi la segnalo adesso che sono tornato e ho pututo rimettere
la testa sulle cose di ricerca.


Mi scuso perch√© nella foga ho commesso una svista, per troppo pessimismo, quindi rimuovendola si guadagna.


Avevo detto che sostituivamo la funzione pseudo-booleana 2^n dimensionale con la sua approssimazione tramite
Shapley/Banzaf (cio√® l‚Äôapprossimazione di grado 1) per cercare l‚Äôinsieme che da il massimo di quest‚Äôultima: ma ci√≤
viene gratis, perch√© essendo quell‚Äôapprossimazione additiva negli atomi, quando hai stimato lo shapley value di quelli,
li metti in ordine e tieni i top k per costruire il ‚Äúdream-team‚Äù dei migliori k elementi.
Quindi questa operazione si riduce al calcolo dello shapley value e non richiede ottimizzazione bayesiana.
Ma per l‚Äôapprossimazione di grado 2 non si pu√≤ usare questa scorciatoia.
Sappiamo che il problema si pu√≤ formulare in termini di grafi (il second‚Äôordine rappresenta l‚Äôinterazione, diciamo
grossomodo lo shapley interaction value) e che la sua soluzione √® equivalente a un problema di min-cut o max-cut.
Questo √® un problema combinatorio degno dell‚Äôottimizzazione Bayesiana.
Forse troviamo gi√† qualcosa sul connubbio dei due. Per√≤ il nostro caso √® speciale, perch√© noi non ci muoviamo alla
cieca come abbiamo fatto nella feature selection.
Come dicevo possiamo contare su: 1) un embedding dello spazio X in cui i punti sono immersi, e anche dello spazio (X,Y),
poi 2) sotto opportune ipotesi possiamo permetterci una mossa esplorativa in cui facciamo il training di un modello
trasparente su tutti i punti.

sto elaborando un un documento word la procedura senza tutti gli ammennicoli che ho aggiunto qui sopra: ne verr√† fuori
un one-pager che condivider√≤

-----

Scusate ho avuto un'illuminazione (che purtroppo rende abbastanza superfluo quel che ho scritto fin qui, per fortuna 
non completamente). Mi spiego.

Nella distillation dobbiamo creare k punti con k<<n in grado di ottimizzare la funzione obiettivo (ad esempio
l‚Äôaccuratezza di un classificatore.
Se assumiamo che ogni punto sperimentale (x,y) abbia un x definito su R^d, allora un insieme di tali punti pu√≤ essere
rappresentato in n x d dimensioni, in R^{n x d}, quindi si pu√≤ ricorrere all‚Äôottimizzazione Bayesiana standard, anche
se alto-dimensionale.
E il risultato della distilation pu√≤ essere usato come punto di partenza per la coreset selection: come coreset
candidato si prendono i punti esistenti pi√π vicini ai prototipi prodotti dalla distillation. Poi si pu√≤ raffinare con
altri metodi.

Quindi abbiamo un approcccio semplice e possiamo cominciare a sperimentare. L'articolo sar√† "A bayesian optimization
approach to dataset distillation". Ho controllato in letteratura e non ho trovato niente che applichi la BO al
problema, quindi possiamo procedere con fiducia.

-----

Un punto importante: le immagini vivono in R^{NxD} dove N √® il numero degli esempi e D il numero delle feature. Ho
letto il papiro fondantivo della distillation (Wang, Zhu, Torralba, Efron, Dataset Distillation) e in effetti loro
osservano che se le feature stanno in R^D e quindi le soluzioni vivono in R^{NxD}, si pu√≤ stimare il gradiente della
loss rispetto allo spazio in questione e quindi si possono applicare metodi standard di gradient based optimization.
Loro non dedicano nemmeno una riga in pi√π al problema e si concentrano sul ridurre il costo dell'inner loop di training.

Quindi con le immagini la Bayesian Optimization √® probabilimente come ammazzare la mosca col cannone. Per√≤ tanto che
ci siamo in confronto con un metodo di libreria (es. Wang et al.) lo possiamo fare: i metodi gradient based effettuano
prevalentemente expoitation, invece la BO introduce l'esploration e questo pu√≤ avere qualche vantaggio.

Per√≤ il passo notevole sarebbe applicarla a problemi con un spazio di feature categoriche. Come capita per la model
selection and hyperparameter optimization in AutoML. L√¨ si che avremmo ragione ad invocare questo approccio!

----

Distanza categorica: 

----
In entrambi i casi, con il decision tree, vedo che con 20 iterazioni si passa da migliaia (mi pare di ricordare) di 
record a 100, e con uno score (forse accuratezza) dell' 80% e 97% rispettivamente. Mi sembra incoraggiante e ho 
chiesto un'estensione a Malerba. 
Vedo due direzioni in cui muoverci: 1. arricchire la caratterizzazione e 2. confrontare con technique alternative. 
- Per la prima cosa si pu√≤ prendere un modello pi√π ricco, tipo random forest che ha pi√π parametri, esplorare un 
numero pi√π alto di iterazioni e caratterizzare la dipendenza dello score dal numero di punti (scendendo gradualmente 
da 100 fino ad un numero di 10 punti, per stressare al massimo il sistema).
- Per la seconda cosa, si possono usare anche solo delle baseline: io penso alla ricerca nello spazio dei parametri 
con random search e gridsearch, ma se si pu√≤ anche il metodo dei centroidi (K-center) ci starebeb bene.

[cm]

Quello che non mi convince e' lo scor alto fin dall'inizio. 
Come acciderbolina fa ad avere uno score alto partendo da dei punti casuali. Boh!
Devo investigare

[gg]

Si hai ragione, se √® alto con 100 pu√≤ darsi che la sfida facile con quella numerosit√†; scendere pi√π essere 
interessante.

il papiro originale della condensation, che usava le imagini del MNIST era sceso a 10

riguardo al secondo punto, se ci sar√† tempo, un altro temine di paragone √®: embedding del discreto in una variet√† 
+ ricerca sul continuo di quella variet√†: siccome l'embedding ha un costo proporzionale al numero di punti se non 
al suo quadrato, la nostra Bayesian Optimization dovrebbe esser competitiva.

Se la dimensionalit√† √® alta, 100 punti non sembrerebbero tanti. E a maggior ragione ottenere un'accuracy alta sin 
dalle primissime iterazioni sembra notevole

S√¨ un po'. I numeri accuracy cos√¨ variabili tra le iterazioni

√à che ad ogni iterazione privilegi l'exploration almeno all' inizio. Se privilegiassi l'exploitation ci sarebbe 
tipicamente un non peggioramento.

[cm]
Devo studiare meglio come "comandare" l'ottimizzatore: ci dovrebbero essere solo 2 parametri su cui agire:


    kappa : float, default: 1.96
        Controls how much of the variance in the predicted values should be
        taken into account. If set to be very high, then we are favouring
        exploration over exploitation and vice versa.
        Used when the acquisition is "LCB".

    xi : float, default: 0.01
        Controls how much improvement one wants over the previous best
        values. Used when the acquisition is either "EI" or "PI".


[gg]
Il secondo √® chiaro. Tu starai usando Expected Improvement. Quindi il primo parametro, che √® il tipico valore 
per avere un intervallo di confidenza del 95% non √® in gioco.

Comunque le fluttuazioni durante l'exploitatoon sono fisiologiche e non me ne preoccuperei.

Deadline della specia issue estesa:
Full paper submission deadline: Extended to March 18, 2024

----

Devo studiare meglio come "comandare" l'ottimizzatore: ci dovrebbero essere solo 2 parametri su cui agire:


    kappa : float, default: 1.96
        Controls how much of the variance in the predicted values should be
        taken into account. If set to be very high, then we are favouring
        exploration over exploitation and vice versa.
        Used when the acquisition is "LCB".

    xi : float, default: 0.01
        Controls how much improvement one wants over the previous best
        values. Used when the acquisition is either "EI" or "PI".

Il secondo √® chiaro. Tu starai usando Expected Improvement. Quindi il primo parametro, che √® il tipico valore per avere 
un intervallo di confidenza del 95% non √® in gioco.

Comunque le fluttuazioni durante l'exploitatoon sono fisiologiche e non me ne preoccuperei.

Deadline della specia issue estesa:
Full paper submission deadline: Extended to March 18, 2024

"Huston, abbiamo un problema":
secondo me, usare BayesOpt OPPURE generare delle soluzioni a caso, otteniamo lo stesso identico risultato.
Dai seguenti plot, non sembra ci sia un "trend" che fa si che l'ottimizzatore baiesiano, dopo un po' inizia a scegliere
soluzioni "piu' buone".
E non c'e' nemmeno un "trend evidente di una soluzione migliore": i miglioramenti nelle performance del classificatore
sono "fondamentalmente" marginali  (qualche digit nelle cifre decimali)

l'ultimo plot usa 100 punti distillati. Non ha un titolo coerente con gli altri perche' e' un vecchio plot. Lo sto
rifacendo, MA per capire di che cosa si tratta, e' sufficiente

non c'√® una variabilit√† troppo alta nella ricerca, cio√® il parametro che controlla l'exploration √® troppo forte?

c'√® una prior gaussiana?


potrebbe non essere una cattiva notizia che trovi l'ottimo dopo solo sei passi; riguardo al confronto con la ricerca
causale e con la ricerca su griglia, quanto ci mettono questo a trovare la soluzione: se si migliora di un ordine di
grandezza √® fatta

volevo dire quanto ci mettono questi?

... nel senso che si pu√≤ avere la stima della facilit√† del problema da quanto tempo di mette la random search o la grid
search, e poi vedere di quale fattore migliora la ricerca con la BO.

GridSearch sul dataset "mushroom": piccolo problemino, con 100 punti, 22 coordinate per punto, quindi 2200 parametri, ci sono

~4*10^1408 (millequattrocentootto)

configurazioni da testare.
Un "tantino tantine" ü§£ 


gi√†, queste configurazioni crescono come funghi! üòÖ

scikit-learn non ha degli oggetti adatti a gestire queste dimensioni.
Ho dovuto implementarne uno custom! 

4*10^1408 salta fuori da tutti i differenti valori che possono assumere le coordinate?

Ho avuto un'illuminazione. In realt√† la dimensionalit√† √® molto pi√π bassa. Permutando i punti si ha lo stesso risultato
(a noi non interessa chi √® il primo chi il secondo, perch√© in questa applicazione non dobbiamo chiamarli per nome)
quindi bisogna dividere per 100! che √® dell'ordine di 10^58. So che √® un miglioramento del 3% ma no si butta via
niente... e magari nei dati ci sono altre simmetrie da sfruttare.

L'illuminazione ha dato il via ad altre illuminazioni (queste sono a basso consumo, quindi non e' che siano cosi' iluminate üòâ):

Partiamo dal datase "Mushroom" fatto solo da colonne categoriche

1) lo spazio e' troppo sparso, quindi serve un modo "ragionevole" per cercare di campionarlo in modo un po' piu' 
intelligente. Pensavo di usare un campinamento in cui, per ogni "dimensione" le etichette vengono scelte INVECE che 
usando una distribuzione "uniforme", mediante una distribuzione "pesata" in base al numero di occorrenze di quella 
etichetta nel dataset originale.
2) un secondo approccio e' quello del "ipercubo latino"

https://en.wikipedia.org/wiki/Latin_hypercube_sampling

pero' bisogna aumentare il numero di campioni, perche' i 100 che uso ora sono decisamente pochi.

3) per fare un campinamento "decente" bisognerebbe provare TUTTE le etichette per TUTTI 2200 parametri ALMENO una volta.

Comunque, "spannometricamente"/"a sensazione" non credo che si possa fare di meglio.


Quindi, poiche' i parametri sono 2200, e testiamo SOLO 100 punti, va da se che e' ragionevole che all'inizio BayesOpt 
non possa essere significativamente meglio di campionamento casuale

Gi√†. All'inizio √® sensato. Temo che il punto si che bisogna andare avanti con BO per un lungo tempo... ci penso.

Ma in dimensionalit√† molto minore gi√† funziona?

il dataset ha 22 colonne. Con 10 punti ci sono 220 parametri. diciamo che servono almeno 1000 iterazioni per vedere 
qualche cambiamento, o forse anche mooolto di piu' (10000?).
Python e' lento: con 100 punti mi pare sto 10min circa. Quindi si passa a 100 o 1000 min.
si puo' fare qualcosa con il parallelismo. Diciamo una divisione per 10, se  c'e' abbastanza memoria

La strategia di campionamento Latin hypercube garantisce uniformit√† della copertura/distribuzione. Il campionamento 
ortogonale garantisce scorrelazione tra le dimensioni. Per√≤ in entrambi i casi stiamo parlando di disegno di 
esperimenti, mentre qui i dati ce li abbiamo gi√† osservati con la loro distribuzione

La suddivisione in intervalli non va fatta uniformemente, ma in base alle coordinate dei data points

Gianni, ricordo che il quadrato latino √® una delle strategie di inizializzazione anche per BO.

Visto che il problema √® la dimensionalit√† pensavi che potremmo provare l'embedding
In una dimensionalit√† non troppo bassa.
Per non buttare via troppa informazione

Ok, immagino che sar√† opportunamente adattato ai dati
Anche io pensavo a riduzione. Ma poi bisogna rimanere nello spazio ridotto

Mi sa che non funziona, almeno se leggo nella mente üòâ
Alcuni numeri: supponiamo il dataset Mushroom fatto solo da colonne categoriche.

1) il dataset ha 22 colonne

2) supponiamo NON un encoding onehot, MA un encoding binario, piu' efficiente. Le colonne diventano 54. Fare dimensional 
reduction qui si potrebbe anche fare MA a quanto la riduci, a 10? Ma poi come ritorni indietro? Perche' bisogna andare 
andare da 10 a 54 (bin encoded) a 22 (properieta' categorichei originali)

3) supponiamo 100 punti per il dataset distillato. Sono 2200 parametri. Qui la dimensional reduction consiste nell'usare 
MENO PUNTI, ma non si puo' andare sotto certi limiti altrimenti le performance del classificatore distillato scende a 
livelli inutili.

4) comprimere i 2200 parametri con quelache sistema di embedding? Il problema e' che non e' un dataset, e' un singolo 
punto in 2200 dimensioni, quindi non si puo' fare. Ma anche se si potesse fare, diciamo un embedding a 100 dimensioni, 
poi comunque bisogna ritornare alle 2200 perche' stiamo cercando i rappresentatin del dataset originale, rappresentato 
dai 2200 parametri cioe' 100 punti con 22 proprieta' ciascuno.

Quindi, boh!

Chiarissimo. Sono d'accordo. Anzi avrei risposto subito gi√† al messaggio di Gianni ieri sera se non avessi avuto paura 
di disturbare. Se per√≤ tu Corrado mi garantisci che metti il cellulare in modalit√† volo risponder√≤ anche la sera.

Nella mia testa sta girando un'idea balenga sull'ottimizzazione in sottospazi, ma prima la preciso poi ve la racconto, 
cos√¨ vediamo se √® da cestinare

Corrado, quando parli di inizializzazione a caso, intendi che ha "generato" dei punti a caso o che li hai scelti a caso
dall'insieme dei punti esistenti?

generati a caso: ogni coordinata viene scelta in modo random tra i possibili valori categorici di quella coordinata

partendo con un sottoinsieme dei dati originali probabilmente si partirebbe avvantaggiati
anzi: fornendo lo score di una collezione di sottoinsiemi del dataset originale si darebbe a BO una visione ampia del
panorama
ciascun campione corrisponde ad un punto den nostro spazio alto-dimensionale e tanti campioni definiranno una zona, una
regione di interesse
mi chiedo se non si possa dire a BO di non allontanarsi troppo da quella regione di interesse (magari si potrebbe
passare questa informazione tramite l'acquisition function se il software consente di metterci le mani sopra)

Secondo me non cambia nulla:
ci sono due approcci (il primo fatto, il secondo da fare)

1) generazione punti random, e l'ottimizzatore cerca le nuove coordinate che saranno sicuramente diverse  dai punti del 
dataset, (praticamente si possono pensare random anche loro)

2) come sopra MA i punti random vengono rimpiazzati da punti del dataset (i piu' vicini)

In teoria si puo' fare, poiche' la funzione di acquisizione e' configurabile. E poi il codice e' disponibile, quindi si 
puo' modificare a piacimento

Sono d'accordo che si tratta di due modalit√† di inizializzazione legittime, ma non sono sicuro che siano equivalenti: 
nel secondo si parte da quantit√† esistenti e il campionamento casuale d√† alcune garanzie sulla plausibilit√† della
soluzione.
Un modo rudimentale per dire a BO di non cercare altrove √® di mettere a zero o a epsilon tutti i valori della curva al
di fuori della regione di interesse (idelmente una sfera che contenga tutti i punti HD fin l√¨ generati - ogniuno
rappresentante 100 punti in LD);

invece della sfera si pu√≤ usare una sfera "sfumata" per consentire un certo allontanamento: questo si implementa non 
mettendo non mettere completamente a zero l'esterno: si moltiplica per una funzione decrescente con la distanza dalla 
superficie o dal centro

ci√≤ limita la ricerca ad un volume moooooolto pi√π piccolo

Anche a me sembrano differenti. Nel caso di ricampionamento dal dataset, si campiona dalla distribuzione empirica dei 
dati, che dovrebbe essere fedele alla distribuzione teorica del fenomeno (supposto che il dataset originale √® stato 
campionato bene)

Nell'altro caso si campiona uniformemente o circa dallo spazio campionario tutto

Un altro tema su cui credo possiamo migliorare √® la codifica dello spazio, perch√© grazie al fatto che il problema √®
degenere (ci sono n! soluzioni identiche grazie alla permutabilit√† dei punti) credo che ci stiamo muovendo in uno
spazio inutilmente ampio. Mi spiego con un esempio giocattolo: se devo trovare l'ottimo, in un cubo HD di ben 3
dimensioni [0,1]^3, e lo faccio usando le sue 3 coordinate nello spazio LD di una dimensione ciascuna definita in [0,1],
non devo consentire a tutti i punti LD di variare sull'intero range [0,1]. Idealmente (dal che se capisce che non ho
un'idea precisa su come codificarlo) si dovrebbe lasciare al primo punto LD x_1 la libert√† di muoversi in [0,1], al
secondo punto x_2 la libert√† di muoversi in [x_1,1], e al terzo x_3 di muoversi in [x_2,1]. Anche generandoli a caso
negli intervalli che ho citato mi sembr a che si renda la ricerca pi√π efficiente. Credo a spanne che il volume dello
spazio di ricerca si riduca ad 1/4 di quello originale. La codifica di questo schema nel nostro caso per√≤ potrebbe non
essere triviale.

